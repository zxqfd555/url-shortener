# Тестирование

Два типа тестирования: тестируем корректность (сервис ведет себя как надо) и тестируем нагрузочно (какой рейт сервис держит).

## Тесты корректности

Файл test_correctness_all.py. Чтоб запустить надо библиотека requests.

Проверяем, что продление ссылок работает. Для этого создаем ссылку с TTL равным 10 секунд и ходим по ней каждую секунду в течении 150 секунд. Удостоверяемся, что код ответа всегда 200. Строго говоря, конечно, 302. Но и браузер и Python requests видя 302 сразу редиректят на оригинальный урл и там уже вернут 200, сходив по нему.

Проверяем, что протухание ссылок работает. Для этого создаем ссылку с TTL равным 10 секунд, 20 секунд по ней не ходим, после чего делаем запрос. Убеждаемся, что там 404.

## Нагрузочные тесты

### Очень много переходов по ссылке

Ссылки на графики и генератор есть в load/get_one_long_link.

Это самый благоприятный кейс.

Запускаем на большом рейте запросов, чтобы сервис упал: https://overload.yandex.net/128226. При падении уперлись в CPU, что видно на графике в Monitoring.

Берем порог, который был незадолго до падения - 7000 RPS. Запускаем с этим постоянным рейтом стрельбу. Результат - работает стабильно. График: https://overload.yandex.net/128228

Итого, без кэширования, а просто долблением монги получилось выжать 7000 RPS, причем с небольшим запасом по CPU.

Начал с 2K RPS. Шаги по оптимизации:
  - Выключил весь логгинг.
  - Эксперименты с размером пула коннектов к монге.
  - Раз уж демон и access log живут в одном процессе, давайте вести лог не построчно, а храня map из slug-а во время последнего посещения.

### Очень много write-ов

Здесь начал генерировать собственные патроны для тестирования. Грабли, на которые можно наступить - забыть в собственных патронах заголовок "Close". Это приведет к очень сильной деградации в тестировании и множеству таймаутов при запросе в тестируемое API. В самом API и MongoDB при этом в логах будет все нормально - проблема видимо в том, что из-за незакрытых соединений мы в какой-то момент не сможем создать новое. Это приведет к таймауту и таймингу в 11 секунд. 11 секунд - стандартный таймаут на запрос в phantom: https://yandextank.readthedocs.io/en/latest/core_and_modules.html#additional-options.

Результаты теста патронах с "Connection: close" на разладку: https://overload.yandex.net/128835. Начинает сыпаться на 6000-6500 RPS. Причина: уперлись в CPU.

Более-менее стабильно работать начинает на 4000 RPS. Ссылка: https://overload.yandex.net/128841. Пики на графике по CPU - запуски демона. Демона можно было бы соптимизировать TTL-индексом на чуть большее время чем ссылка могла бы жить, но и при этом наблюдаем вполне достойный результат.

### Имитация "реального мира"

Ссылки на графики и генератор есть в load/real_world.

Нормально сымитировать, на самом деле, сложно.

Делаем 1% запросов на запись и 99% запросов на чтение. Читаем какое-то небольшое множество страниц, потому что кейс посещения за 5-10 секунд  __всех__ страниц коллекции - что-то странное. Берем id страниц из Пуассоновского распределения.

Еще одна проблема: нам в патронах надо знать заранее URL, который сгенерируется. А мы не можем, он рандомный. Чтобы это обойти, реализовал интерфейс генераторов slug-ов и использовал Sequential: https://github.com/zxqfd555/url-shortener/blob/master/src/lib/slug.h#L32

По итогу вышло неплохо. Все ломается на около 7К из-за CPU. Графики: https://overload.yandex.net/128843. При 6K работало на грани - 99.5-перцентиль имела время около 20 ms.

При 5500 RPS результат увереннее: https://overload.yandex.net/128845. Пик - уход в ждущий режим. Так и не настроил его, а ждать такое время за ноутбуком не хотелось. Отсюда делаю вывод, что сервис держит 5500 RPS. Также, при отсутствии бешенного рейта записей запуски демона на графике CPU начинают быть не столь видны.

## Выводы по итогу нагрузочного тестирования

Экспериментально еще раз убедились, что запись медленне чтения. С помощью более умного подхода к продлению ссылок удалось выжать дополнительные 1500 RPS по сравнению с кейсом "только записи", а ведь можно было и еще оптимизировать.

В каких-то идеальных случаях можно держать до 7K, но не уверен, что это особо показательно. Надо исходить из планируемой нагрузки и возможно в дальнейших тестах уменьшать еще больше число write-ов, но все же не убирать их вовсе.

Можно было еще прикрутить кэш, но тогда в моем конкретном случае (не распределенном), все свелось бы либо к искуственному ограничению размера кэша, либо к полному in-memory. Оба варианта не особо интересны, при том связкой "Crow+MongoDB" удалось на обычном ноутбуке выжать тысячи RPS. Но есть понимание, что с кэшом можно еще лучше.

4-5 минут в запусках нам хватает, потому что неординарную нагрузку, дополнительную к обычной нам может дать только запуск демона. Он запускается раз в 90 секунд, следовательно, мы переживаем примерно три таких пика.
