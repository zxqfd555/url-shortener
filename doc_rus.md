# Постановка задачи

Необходимо разработать сокращалку URL-ов (как t.co или goo.gl). Должны быть поддержаны следующие функции:

  - Добавить ссылку. Для нее необходимо вернуть сокращенный URL, то есть URL вида "sh.co/shrt". Для ссылки задается время, которое она живет на сервере с момента последнего доступа.
  - Проследовать по сокращенной ссылке. Необходимо перенаправить пользователя по актуальному URL-у, либо вернуть 404 если ссылки нет, либо она была удалена вследствие протухания.

# API

Два endpoint-а.

Подход: недо-REST. От REST мы возьмем глаголы в методе запроса, statelessness. HATEOAS как "карту" методов API реализовывать не будем, так как оно достаточно простое.

## Добавить сокращенную ссылку

POST `/link`
Тело запроса - JSON с полями:
  - `"url"` -- адрес ссылки;
  - `"timeout"` -- гарантированное время жизни ссылки после добавления или последнего доступа к ней (whatever happens last) в секундах.

Варианты ответа:
  - Успех. Код 201, в заголовке `Location` - сокращенный URL, вместе с хостом (то есть, например: "sh.co/shrt").
  - Плохой формат запроса. Код 400. Если в JSON-е в теле нет поля `"url"` или `"timeout"`.

## Проследовать по сокращенной ссылке

GET `/<slug>`

Варианты ответа:

  - Успех. Код 301, в заголовке `Location` - реальный URL. То есть, перенаправление по актуальному URL-у.
  - Такой ссылки нет. Код 404. Это могло случиться по двум причинам: по этой ссылке никогда ничего и не было или же истек дедлайн с времени последнего доступа.

# CAP и архитектура

Будем целиться в CP-гарантии. Мотивация: когда пользователь сокращает ссылку, он зачастую сразу хочет поделиться ей с кем-то. Поэтому важно, чтобы по сокращенной ссылке доступ был сразу же.

С другой стороны, мы не будем пытаться добиться того, чтобы ссылка исчезала в точности после выставленного дедлайна. Мы разрешим ссылке жить еще недолго после протухания, давая гарантию лишь на то, что если не было окна длиной в TTL, в котором обращений по ссылке не было, то ссылка всегда будет не в удаленном состоянии. Если такое окно уже прошло, а ссылку мы удалить не успели, и по ней походили, то мы все также будем стараться ее продлить, однако 100%-гарантий уже не дается. Позже покажем, что наше решение имеет модель консистентности eventual consistency.

Дальше рассмотрим два решения, из которых возьмем второе.
В качестве БД для хранения основных данных возьмем MongoDB, настроенную на CP-гарантии с использованием механизма консенсусов.

В MongoDB одна коллекция - описание сокращенных ссылок. Документы в этой коллекции - отображения из slug (короткого URL) в следующие поля:

  - **expiration_timestamp**. Целое число, UTC timestamp того момента, когда ссылка начнет считаться протухшей. По этому полю имеем индекс для быстрого выбора всех записей в каком-то полуинтервале. Может меняться.
  - **ttl**. Сколько в секундах ссылка должна гарантированно жить после последнего обращения.
  - **url**. URL того ресурса, на который ссылка должна вести.
  
## Первое решение

Каждый раз, когда приходит запрос на запись - генерируем slug. Генерируем случайным образом, взяв шесть случайных символов из множества маленьких, больших английских букв и цифр. Получаем 26+26+10 = 62 варианта символа на каждой позиции или 56,800,235,584 вариантов ссылок. Будем действовать следующим образом: сгенерируем slug, если такой в базе есть, то будем повторять, пока не найдем такой, которого в базе нет. Как только валидный slug есть - просто пишем в нашу коллекцию новый маппинг: 
  slug -> {
    expiration_timestamp: текущий timestamp + ttl,
    ttl: заданный нам TTL ссылки в секундах,
    url: URL ресурса, на который должна вести ссылка
  }

Каждый раз, когда приходит запрос - достаем ссылку по slug-у и если она протухла (expiration_timestamp < текущий timestamp), то отдаем 404. Иначе - отдаем результат и меняем по ключу slug-а значение expiration_timestamp на (текущий timestamp) + ttl.

Регулярным фоновым процессом удаляем и все остальные просроченные ссылки, выбирая те, у которых expiration_timestamp < (текущий timestamp).

## Второе решение

Первое решение плохо тем, что на каждую операцию получения ссылки у нас идет запись в БД, которая более тяжелая, чем чтение.

Для того, чтобы этого избежать, предлагается завести регулярный процесс, который будет действовать следующим образом: раз в 5 минут со всех инстансов собираются логи (например, логи nginx). В сложной распределенной системе логи могут доезжать в распределенное хранилище, например. Затем, по этим логам, например, с помощью MapReducе можно построить для slug-ов которые за последние пять минут встретились отображение во времена последнего использования. Затем, согласно этим временам можно в базе продлить время жизни этим объектам.

Тогда добавление ссылки никак не меняется по сравнению с предыдущим решением. Переход по ссылке будет обрататываться следующим образом:

  - Достаем из MongoDB для slug-a структуру с информацией о ссылке. Если такой нет - 404. Если ссылка уже протухла - 404.
  - Если ссылке осталось жить меньше, чем (регулярность процесса по обработке логов) + (время работы процесса по обработке логов), то продлеваем ее время жизни прямой записью в MongoDB и редиректим по оригинальному URL-у.
  - Если ссылке осталось жить больше, чем обозначенное выше время, то о продлении срока жизни волноваться не стоит - это сделает регулярный процесс на следующей своей итерации. Просто редиректим по оригинальному URL-у.
  
Таким образом получается сократить число DB write-ов, особенно, если TTL-ы в среднем не очень маленькие (дни).

Если предположить, что запросы к системе останавливаются, то данные придут в консистентное состояние - следующим проходом регулярного процесса все устаревшие ссылки вычистятся и that's it. То есть имеет модель консистентности eventual consistency.

## Дальнейшая оптимизация

Чтобы снизить нагрузку, можно попробовать поставить перед бэкэндами инстанс с Redis, который будет использоваться для LRU-кэш для последних 1-10 миллионов ссылок. Если он падает - ничего страшного, ходим напрямую на бэкэнды как в прошлой схеме.

Базу можно шардировать для снижения нагрузки и в фоне реплицировать для улучшения доступности.
